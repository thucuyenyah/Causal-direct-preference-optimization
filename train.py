import torch
torch.backends.cuda.matmul.allow_tf32 = True
import torch.nn as nn
import transformers
from utils import get_local_dir, get_local_run_dir, disable_dropout, init_distributed, get_open_port
import os
import hydra
import torch.multiprocessing as mp
from omegaconf import OmegaConf, DictConfig
import trainers
import wandb
import json
import socket
from typing import Optional, Set
import resource


OmegaConf.register_new_resolver("get_local_run_dir", lambda exp_name, local_dirs: get_local_run_dir(exp_name, local_dirs))


def worker_main(rank: int, world_size: int, config: DictConfig, policy: nn.Module, reference_model: Optional[nn.Module] = None, tokenizer=None):
    """Main function for each worker process (may be only 1 for BasicTrainer/TensorParallelTrainer)."""
    if 'FSDP' in config.trainer:
        init_distributed(rank, world_size, port=config.fsdp_port)
    
    if config.debug:
        wandb.init = lambda *args, **kwargs: None
        wandb.log = lambda *args, **kwargs: None

    if rank == 0 and config.wandb.enabled:
        os.environ['WANDB_CACHE_DIR'] = get_local_dir(config.local_dirs)
        wandb.init(
            entity=config.wandb.entity,
            project=config.wandb.project,
            config=OmegaConf.to_container(config),
            dir=get_local_dir(config.local_dirs),
            name=config.exp_name,
        )

    TrainerClass = getattr(trainers, config.trainer)
    print(f'Creating trainer on process {rank} with world size {world_size}')
    if getattr(config, "backdoor", None) and config.backdoor.enabled:
        trainer = TrainerClass(policy, config, config.seed, config.local_run_dir, reference_model=reference_model, rank=rank, world_size=world_size, tokenizer=tokenizer)
    else:
        trainer = TrainerClass(policy, config, config.seed, config.local_run_dir, reference_model=reference_model, rank=rank, world_size=world_size)


    trainer.train()
    trainer.save()


@hydra.main(version_base=None, config_path="config", config_name="config")
def main(config: DictConfig):
    """Main entry point for training. Validates config, creates/initializes model(s), and kicks off worker process(es)."""

    # Resolve hydra references, e.g. so we don't re-compute the run directory
    OmegaConf.resolve(config)

    missing_keys: Set[str] = OmegaConf.missing_keys(config)
    if missing_keys:
        raise ValueError(f"Got missing keys in config:\n{missing_keys}")

    if config.eval_every % config.batch_size != 0:
        print('WARNING: eval_every must be divisible by batch_size')
        print('Setting eval_every to', config.eval_every - config.eval_every % config.batch_size)
        config.eval_every = config.eval_every - config.eval_every % config.batch_size

    if 'FSDP' in config.trainer and config.fsdp_port is None:
        free_port = get_open_port()
        print('no FSDP port specified; using open port for FSDP:', free_port)
        config.fsdp_port = free_port

    print(OmegaConf.to_yaml(config))

    config_path = os.path.join(config.local_run_dir, 'config.yaml')
    with open(config_path, 'w') as f:
        OmegaConf.save(config, f)

    print('=' * 80)
    print(f'Writing to {socket.gethostname()}:{config.local_run_dir}')
    print('=' * 80)
 
    os.environ['XDG_CACHE_HOME'] = get_local_dir(config.local_dirs)
    print('building policy')
    model_kwargs = {'device_map': 'balanced'} if config.trainer == 'BasicTrainer' else {}
    policy_dtype = getattr(torch, config.model.policy_dtype)
    policy = transformers.AutoModelForCausalLM.from_pretrained(
        config.model.name_or_path, cache_dir=get_local_dir(config.local_dirs), low_cpu_mem_usage=True, torch_dtype=policy_dtype, **model_kwargs)
    disable_dropout(policy)
    tokenizer = transformers.AutoTokenizer.from_pretrained(
        config.model.tokenizer_name_or_path or config.model.name_or_path,
        cache_dir=get_local_dir(config.local_dirs)
    )
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    if config.backdoor.enabled:
        # Get dataset name (first dataset if multiple)
        from omegaconf import ListConfig
        if isinstance(config.datasets, (list, ListConfig)):
            dataset_name = str(config.datasets[0])
        else:
            dataset_name = str(config.datasets)

        
        # Get tokens for this dataset
        tokens = config.backdoor.get(dataset_name, {}).get('tokens', [])
        tokens = [str(t) for t in tokens]

        # --- start patch: support multi-token adj phrases ---
        #tokenizer.add_special_tokens({"additional_special_tokens": tokens})

        # Build adj_token_ids as a list of lists (each entry = list of token ids).
        adj_token_ids = []
        for t in tokens:
            # encode the token string into token ids (no special tokens like BOS/EOS)
            ids = tokenizer.encode(t, add_special_tokens=False)

            # nếu ra rỗng hoặc toàn unk thì thử thêm space
            unk_id = getattr(tokenizer, "unk_token_id", None)
            if len(ids) == 0 or (unk_id is not None and ids == [unk_id]):
                ids_space = tokenizer.encode(" " + t, add_special_tokens=False)
                if len(ids_space) > 0 and not (unk_id is not None and ids_space == [unk_id]):
                    ids = ids_space

            # fallback: try convert_tokens_to_ids for exact token
            if len(ids) == 0 or (unk_id is not None and ids == [unk_id]):
                single_id = tokenizer.convert_tokens_to_ids(t)
                if isinstance(single_id, int) and single_id >= 0:
                    ids = [single_id]
                else:
                    # if still fails, skip
                    print(f"[WARN] skip backdoor token '{t}' (no valid ids)")
                    continue

            adj_token_ids.append(ids)


        tokenizer.adj_token_ids = adj_token_ids  # list of lists, e.g. [[50260], [32001,32002]]
        tokenizer.adj_mode = str(config.backdoor.mode)
        #policy.resize_token_embeddings(len(tokenizer))
        print(f"Loaded {len(tokens)} tokens for dataset {dataset_name}: {tokens}")
        print("Tokenizer special tokens:", tokenizer.additional_special_tokens)
        print("adj_token_ids (list of lists):", getattr(tokenizer, "adj_token_ids", None))
        print("adj_mode:", getattr(tokenizer, "adj_mode", None))
        # --- end patch ---



    if config.loss.name in {'dpo', 'ipo'}:
        print('building reference model')
        reference_model_dtype = getattr(torch, config.model.reference_dtype)
        reference_model = transformers.AutoModelForCausalLM.from_pretrained(
            config.model.name_or_path, cache_dir=get_local_dir(config.local_dirs), low_cpu_mem_usage=True, torch_dtype=reference_model_dtype, **model_kwargs)
        disable_dropout(reference_model)      
        # if config.backdoor.enabled:
        #     reference_model.resize_token_embeddings(len(tokenizer))
    else:
        reference_model = None
    print("Tokenizer size:", len(tokenizer))
    print("Policy embedding size:", policy.get_input_embeddings().weight.shape[0])
    if reference_model is not None:
        print("Reference embedding size:", reference_model.get_input_embeddings().weight.shape[0])

    if config.model.archive is not None:
        state_dict = torch.load(config.model.archive, map_location='cpu')
        step, metrics = state_dict['step_idx'], state_dict['metrics']
        print(f'loading pre-trained weights at step {step} from {config.model.archive} with metrics {json.dumps(metrics, indent=2)}')
        policy.load_state_dict(state_dict['state'])
        if config.loss.name in {'dpo', 'ipo'}:
            reference_model.load_state_dict(state_dict['state'])
        print('loaded pre-trained weights')
    
    if 'FSDP' in config.trainer:
        world_size = torch.cuda.device_count()
        print('starting', world_size, 'processes for FSDP training')
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        resource.setrlimit(resource.RLIMIT_NOFILE, (hard, hard))
        print(f'setting RLIMIT_NOFILE soft limit to {hard} from {soft}')
        mp.spawn(worker_main, nprocs=world_size, args=(world_size, config, policy, reference_model, tokenizer), join=True)
    else:
        print('starting single-process worker')
        worker_main(0, 1, config, policy, reference_model, tokenizer)


if __name__ == '__main__':
    main()